defaults:
- _self_
- model: blank_model
- sched: WarmupHoldPolicy

# name for this experiment in the local run directory
exp_name: moe_trial

config_path: null

# random seed for batch sampling
seed: 0

# the batch size for training; for FSDP, the batch size per GPU is batch_size / (grad_accumulation_steps * num_gpus)
per_device_train_batch_size: 1
global_train_batch_size: ${get_global_batch_size:${per_device_train_batch_size}}

# the batch size during evaluation and sampling, if enabled
per_device_eval_batch_size: ${per_device_train_batch_size}
global_eval_batch_size: ${get_global_batch_size:${per_device_eval_batch_size}}

max_grad_norm: 0.

max_steps: 10
use_synthetic_data: true

label_pad_token_id: -100
pad_token_id: -100

# which dataset(s) to train on; can pass a list like datasets=[hh,shp]
# including hh, shp, or os
datasets: hh

beta: 0.1

output_dir: /tmp

# whether to eval at the very beginning of training
do_first_eval: false

# an OmegaConf resolver that returns the local run directory, calling a function in utils.py
run_dir: ${path_join:${output_dir},${exp_name}}

# the learning rate
lr: 5e-7

# number of steps to accumulate over for each batch
#   (e.g. if batch_size=4 and gradient_accumulation_steps=2, then we will
#   accumulate gradients over 2 microbatches of size 2)
# gradient_accumulation_steps: 1

# the maximum allowed length for an input (prompt + response)
max_length: 512

# the maximum allowed length for a prompt
max_prompt_length: 256

# the number of examples to evaluate on (and sample from, if sample_during_eval is true)
n_eval_examples: null

# The optimizer to use; we use RMSprop because it works about as well as Adam and is more memory-efficient
optimizer: ADAMW_TORCH_XLA

# evaluate and save model every eval_every steps
eval_frequency: 3

# num of process in data processing
num_proc: 1

# combine forward
concatenated_forward: True

# path to load checkpoint
checkpoint_manager_path: null

# dry run experiment with a subset of dataset
dry_run: False

# shuffle train data set
shuffle: True

# use float32 in matmul in torch xla
full_precision: False

# path to save compile cache for torch xla
local_compile_cache_dir: null

# tensor_parallelism and fsdp parallelism would be num_devices / tensor_parallelism
tensor_parallelism: 1

# whether to load dataset from cache
load_from_cache_file: True

# report frequency of train step
report_metrics_freq: 1

# cache of models
cache_local_dir: null

hydra:
  run:
    dir: ${run_dir}